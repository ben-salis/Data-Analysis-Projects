{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"a42854da-13f2-4e6f-bf98-5e639b9cee3a","_cell_guid":"17a99831-aa47-416b-b6dd-1606ad53516a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Purpose\n\nCurrently, I'm working on a side project using NYC COVID-19 data provided by the DOHMH. It's a time series spanning the last several years. The current structure is suitable for my needs but, as I was working on it, I wondered at restructuring. \n\nThe data is wide and makes my analyses tedious. This doesn't need to be the case. Since the dataset is large, spreadsheets lag too much to be very useful. A Python script made the most sense to me. \n\nA longer dataset with less columns. That's the idea. It would certainly clean my SQL queries up. Maybe I'll generalize the process here in case I need it again. I don't anticipate needing more than a few loops. We'll see. \n\n**Ben Salis**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"48f64a58-dd38-4939-8135-1fc63629ad68","_cell_guid":"cb4e7335-0083-4004-b3eb-a7d7d331a556","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:54:54.367835Z","iopub.execute_input":"2023-09-23T19:54:54.368459Z","iopub.status.idle":"2023-09-23T19:54:54.794186Z","shell.execute_reply.started":"2023-09-23T19:54:54.368393Z","shell.execute_reply":"2023-09-23T19:54:54.792971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_count = pd.read_csv(\"/kaggle/input/covid-19-daily-counts-nyc/COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\")","metadata":{"_uuid":"fce9d4b7-9ac6-41e4-8f23-525ad7bb3ee7","_cell_guid":"020553ae-9165-4b91-945e-8960f7580ed7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:54:57.307522Z","iopub.execute_input":"2023-09-23T19:54:57.308088Z","iopub.status.idle":"2023-09-23T19:54:57.349451Z","shell.execute_reply.started":"2023-09-23T19:54:57.308030Z","shell.execute_reply":"2023-09-23T19:54:57.347807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_count.head()\ncov_header = list(cov_count.columns)\nprint(len(cov_header))","metadata":{"_uuid":"f5f84233-6eac-44cc-8f87-59ede8a6d02f","_cell_guid":"61a306f9-905e-4e7e-8952-4fb2fe6d8cd7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:55:02.506091Z","iopub.execute_input":"2023-09-23T19:55:02.506610Z","iopub.status.idle":"2023-09-23T19:55:02.513984Z","shell.execute_reply.started":"2023-09-23T19:55:02.506567Z","shell.execute_reply":"2023-09-23T19:55:02.512410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only want specific attributes for now. Culling this list should make my cases simpler.\nfor item in cov_header:\n    if 'PROBABLE' in item: #I want more information on these columns. \n        cov_header.remove(item)\n    elif 'AVG' in item:\n        cov_header.remove(item)\n        \nprint(cov_header)\nprint(len(cov_header))","metadata":{"_uuid":"944526d8-d996-48ec-9e82-885e566e6b9d","_cell_guid":"7b4a59b2-a156-46c7-8979-9d6ac5b6a8f1","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T00:38:22.359704Z","iopub.execute_input":"2023-09-23T00:38:22.360466Z","iopub.status.idle":"2023-09-23T00:38:22.366411Z","shell.execute_reply.started":"2023-09-23T00:38:22.360428Z","shell.execute_reply":"2023-09-23T00:38:22.365042Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_df = [] # list that will be made into a dataframe by the end","metadata":{"_uuid":"b6d309aa-851d-41b4-8bdc-87726b17ee8d","_cell_guid":"af17c0c1-e7dd-421d-8b55-cc1d40acb6dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:55:31.938556Z","iopub.execute_input":"2023-09-23T19:55:31.939082Z","iopub.status.idle":"2023-09-23T19:55:31.944815Z","shell.execute_reply.started":"2023-09-23T19:55:31.939023Z","shell.execute_reply":"2023-09-23T19:55:31.943552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nUsing this nested loop to restructure the raw data. \nHonestly, making a method might be worthwhile. This\nisn't the first time I've needed to pivot a large dataset\nin this way. \n'''\nfor elem in cov_header:\n    for row in range(len(cov_count)):\n        date = cov_count['date_of_interest'][row]\n        val = cov_count[elem][row]\n        if elem.startswith('SI') == True: #could use \"in\" operator. \n            borough = 'Staten Island'\n            if 'CASE' in elem:\n                category = 'Cases'\n                pre_df += [(date,borough,category,val)]\n            elif 'DEATH' in elem:\n                category = 'Deaths'\n                pre_df += [(date,borough,category,val)]\n            elif 'HOSPITALIZED' in elem:\n                category = 'Hospitalized'\n                pre_df += [(date,borough,category,val)]","metadata":{"_uuid":"ac2385c7-fc4f-4ec4-a574-db62e98d0a11","_cell_guid":"b12ef0c5-48fd-4f78-8400-ad70ee207b82","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:42:11.602301Z","iopub.execute_input":"2023-09-23T01:42:11.602705Z","iopub.status.idle":"2023-09-23T01:42:11.937001Z","shell.execute_reply.started":"2023-09-23T01:42:11.602669Z","shell.execute_reply":"2023-09-23T01:42:11.936052Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(pre_df))","metadata":{"_uuid":"0186e553-414c-4668-a763-1518125f4677","_cell_guid":"068a6101-b53d-453b-a9de-4d4ca276f030","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:42:15.193878Z","iopub.execute_input":"2023-09-23T01:42:15.194251Z","iopub.status.idle":"2023-09-23T01:42:15.199716Z","shell.execute_reply.started":"2023-09-23T01:42:15.194218Z","shell.execute_reply":"2023-09-23T01:42:15.198493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(pre_df, columns= [\"Date\",\"Location\", \"Classification\", \"Count\"])\ndf #this long data format is better for SQL and Tableau. The wide version wasn't great\n\n# Note: Could keep analysis in Python, but this script is primarily a proof of concept","metadata":{"_uuid":"e410ae29-5705-4f8a-a0cd-3ecaad7f6624","_cell_guid":"1cda0d78-1dbd-4bcd-8572-02d35067df26","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:43:57.835095Z","iopub.execute_input":"2023-09-23T01:43:57.835664Z","iopub.status.idle":"2023-09-23T01:43:57.903184Z","shell.execute_reply.started":"2023-09-23T01:43:57.835619Z","shell.execute_reply":"2023-09-23T01:43:57.901926Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('COVID19_count_long.csv',index = False) ","metadata":{"_uuid":"4be90575-c553-4f03-a49e-01ce85af97fb","_cell_guid":"28689eef-5469-46ae-abfd-f75295a0244b","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:47:28.886376Z","iopub.execute_input":"2023-09-23T01:47:28.887108Z","iopub.status.idle":"2023-09-23T01:47:28.929165Z","shell.execute_reply.started":"2023-09-23T01:47:28.887059Z","shell.execute_reply":"2023-09-23T01:47:28.928190Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}