{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Purpose\n\nCurrently, I'm working on a side project using NYC COVID-19 data provided by the DOHMH. It's a time series spanning the last several years. The current structure is suitable for my needs but, as I was working on it, I wondered at restructuring. \n\nThe data is wide and makes my analyses tedious. This doesn't need to be the case. Since the dataset is large, spreadsheets lag too much to be very useful. A Python script made the most sense to me. \n\nA longer dataset with less columns. That's the idea. It would certainly clean my SQL queries up. Maybe I'll generalize the process here in case I need it again. I don't anticipate needing more than a few loops. We'll see. \n\n**Ben Salis**","metadata":{"_uuid":"48525a22-c9a1-4738-861c-db6937f7297b","_cell_guid":"d18f606c-4d43-438e-9bd2-b9e93c9c4735","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"f0d929fc-9f02-43f4-b7c8-86d57ce609b5","_cell_guid":"4a31c1b1-1920-41e9-bc20-9167be5795ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:54:54.367835Z","iopub.execute_input":"2023-09-23T19:54:54.368459Z","iopub.status.idle":"2023-09-23T19:54:54.794186Z","shell.execute_reply.started":"2023-09-23T19:54:54.368393Z","shell.execute_reply":"2023-09-23T19:54:54.792971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_count = pd.read_csv(\"/kaggle/input/covid-19-daily-counts-nyc/COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\")","metadata":{"_uuid":"1e78b638-771d-48d9-91c1-b6ea407df0f2","_cell_guid":"577340d7-a339-4b81-b3fd-ef358b60019d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:54:57.307522Z","iopub.execute_input":"2023-09-23T19:54:57.308088Z","iopub.status.idle":"2023-09-23T19:54:57.349451Z","shell.execute_reply.started":"2023-09-23T19:54:57.308030Z","shell.execute_reply":"2023-09-23T19:54:57.347807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_count.head()\ncov_header = list(cov_count.columns)\nprint(len(cov_header))","metadata":{"_uuid":"3b2b55b2-2a44-41df-b490-31902238fb52","_cell_guid":"e82df353-3d17-4d5b-b9af-eb4629b50a9f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:55:02.506091Z","iopub.execute_input":"2023-09-23T19:55:02.506610Z","iopub.status.idle":"2023-09-23T19:55:02.513984Z","shell.execute_reply.started":"2023-09-23T19:55:02.506567Z","shell.execute_reply":"2023-09-23T19:55:02.512410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only want specific attributes for now. Culling this list should make my cases simpler.\nfor item in cov_header:\n    if 'PROBABLE' in item: #I want more information on these columns. \n        cov_header.remove(item)\n    elif 'AVG' in item:\n        cov_header.remove(item)\n        \nprint(cov_header)\nprint(len(cov_header))","metadata":{"_uuid":"7620cea2-5600-4d51-8050-cca05f3b126b","_cell_guid":"b444e89a-c9ed-4c58-b749-696945d08fe3","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T00:38:22.359704Z","iopub.execute_input":"2023-09-23T00:38:22.360466Z","iopub.status.idle":"2023-09-23T00:38:22.366411Z","shell.execute_reply.started":"2023-09-23T00:38:22.360428Z","shell.execute_reply":"2023-09-23T00:38:22.365042Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_df = [] # list that will be made into a dataframe by the end","metadata":{"_uuid":"788ae571-c0bb-4479-a475-736cb950964f","_cell_guid":"279ed647-8567-4a86-967c-72f3e4c011fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-23T19:55:31.938556Z","iopub.execute_input":"2023-09-23T19:55:31.939082Z","iopub.status.idle":"2023-09-23T19:55:31.944815Z","shell.execute_reply.started":"2023-09-23T19:55:31.939023Z","shell.execute_reply":"2023-09-23T19:55:31.943552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nUsing this nested loop to restructure the raw data. \nHonestly, making a method might be worthwhile. This\nisn't the first time I've needed to pivot a large dataset\nin this way. \n'''\nfor elem in cov_header:\n    for row in range(len(cov_count)):\n        date = cov_count['date_of_interest'][row]\n        val = cov_count[elem][row]\n        if elem.startswith('SI') == True: #could use \"in\" operator. \n            borough = 'Staten Island'\n            if 'CASE' in elem:\n                category = 'Cases'\n                pre_df += [(date,borough,category,val)]\n            elif 'DEATH' in elem:\n                category = 'Deaths'\n                pre_df += [(date,borough,category,val)]\n            elif 'HOSPITALIZED' in elem:\n                category = 'Hospitalized'\n                pre_df += [(date,borough,category,val)]","metadata":{"_uuid":"5a3dd6f8-0aeb-426b-ae28-df0e0adde6e5","_cell_guid":"c5ab0aef-9992-4290-9b06-c3fa6b813ad7","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:42:11.602301Z","iopub.execute_input":"2023-09-23T01:42:11.602705Z","iopub.status.idle":"2023-09-23T01:42:11.937001Z","shell.execute_reply.started":"2023-09-23T01:42:11.602669Z","shell.execute_reply":"2023-09-23T01:42:11.936052Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(pre_df))","metadata":{"_uuid":"0af85ef7-fd8b-4ddd-b4ac-5296947bc0bb","_cell_guid":"bd725f16-c355-4daa-8bed-1f71c1583836","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:42:15.193878Z","iopub.execute_input":"2023-09-23T01:42:15.194251Z","iopub.status.idle":"2023-09-23T01:42:15.199716Z","shell.execute_reply.started":"2023-09-23T01:42:15.194218Z","shell.execute_reply":"2023-09-23T01:42:15.198493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(pre_df, columns= [\"Date\",\"Location\", \"Classification\", \"Count\"])\ndf #this long data format is better for SQL and Tableau. The wide version wasn't great\n\n# Note: Could keep analysis in Python, but this script is primarily a proof of concept","metadata":{"_uuid":"8971f6cf-73f6-4d4f-8519-5f4e2cd8c7ee","_cell_guid":"1f5da709-2eca-43b4-b66f-1d51ab71a69d","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:43:57.835095Z","iopub.execute_input":"2023-09-23T01:43:57.835664Z","iopub.status.idle":"2023-09-23T01:43:57.903184Z","shell.execute_reply.started":"2023-09-23T01:43:57.835619Z","shell.execute_reply":"2023-09-23T01:43:57.901926Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('COVID19_count_long.csv',index = False)","metadata":{"_uuid":"c00bc53a-3d87-403c-943b-10b6fdbf9be7","_cell_guid":"3957fedf-9bba-4c53-8f78-e5010262d3a1","collapsed":false,"execution":{"iopub.status.busy":"2023-09-23T01:47:28.886376Z","iopub.execute_input":"2023-09-23T01:47:28.887108Z","iopub.status.idle":"2023-09-23T01:47:28.929165Z","shell.execute_reply.started":"2023-09-23T01:47:28.887059Z","shell.execute_reply":"2023-09-23T01:47:28.928190Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}